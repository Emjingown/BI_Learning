## Thinking1：在CTR点击率预估中，使用GBDT+LR的原理是什么？

* 采用了stacking的方法，即构建一种分层模型集成框架，采用串行的结合方法
* GBDT+LR包含两层模型，其中第一层的GBDT对原始数据进行处理，自动进行特征组合与提取；进而，再将提取后的数据代入到LR模型中进行进一步拟合，从而提高整体模型的预测准确度



## Thinking2：Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）

* Wide & Deep模型采用的是将**LR模型与DNN模型进行并行结合**的结构

* **记忆能力**

  * 看做从历史中学习到的比较显示的相关性
  * 我的理解是可以基于对业务、问题的理解而提炼出来的一些与预测结构比较相关的关键特征
  * 通过对原始数据的直接使用与适当的人工特征组合，再通过LR模型可得到一个初步的更具有解释性的结果

* **泛化能力**

  * 相关性的一个延续。探索过去较少或者从未发生的新特征组合
  * 我的理解是一些物品背后具有更深层次、更复杂的联系，这种联系（隐式特征）很难通过人工去理解和提取
  * 通过利用DNN本身能够进行自动地、复杂地特征组合，来获取这种潜在特征，从而推荐一些有潜在相关性、但无法人工发现的商品，从而使推荐结果更具有多样性

  

## Thinking3：在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？

* 采用并行方式结合的模型，如DeepFM模型、Wide&Deep模型

* 采用串行方式结合的模型，如FNN模型、NFM模型

  


## Thinking4：GBDT和随机森林都是基于树的算法，它们有什么区别？

* **随机森林**
  * 采用的是**Bagging**的思想，弱学习器（决策树）之间是并行的关系
  * 通过对训练集进行有放回的采样，得到多个不同的子训练集，进而分别训练出不同的决策树模型，最后再将不同决策树模型的预测结果采用“投票”的方法得到最终结果（分类问题少数服从多数，回归问题取平均）
* **GBDT**
  * 采用的是**Stacking**的思想，弱学习器之间是串行的关系
  * 除第一棵树直接对原始数据进行拟合以外，后续每一颗树都是基于上一棵树的预测残差，再进行迭代拟合，使最终的loss尽可能小
  * 最终的结果为最后一棵树得到的结果



## Thinking5：item流行度在推荐系统中有怎样的应用

* 热门推荐，解决冷启动问题
* 已经有大量行为，作为权重（越流行，权重越低） 


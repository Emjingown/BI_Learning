## Thinking1：奇异值分解SVD的原理是怎样的，都有哪些应用场景

1. **原理**

   * 一般应用于非对称矩阵
   
   * 若矩阵$A$的维度为$m*n$，则$AA^T$和$A^TA$都是**对称方阵**，可得到
     $$
     AA^T=P \Lambda_1P^T, A^TA = Q \Lambda_2 Q^T
     $$
   
     * 两者具有**相同的非零特征值**，**是矩阵$A$对应特征值的平方**
     * 此时方便求取两个方阵的特征值与特征向量
   
   * 进而，矩阵$A$可分解为$A=P \Lambda Q^T$，对上述特征值进行开方即可得到$A$的特征值
   
2. **应用场景**

   * 降维
   * 图像压缩
   * 推荐系统（将分解后的三个矩阵进行适当变换，即可得到两个矩阵相乘的形式，类似用户与商品的关系）



## Thinking2：FunkSVD, BiasSVD，SVD++算法之间的区别是怎样的

* **FunkSVD**
  
  * 在传统SVD的基础上
    
    * 通过设置保留的特征值个数$k$，来对矩阵进行近似求解，减少计算量
    * 只关注原矩阵有值位置的对比，来降低噪声影响
    * 将分解后的三个矩阵转化成两个矩阵相乘，也可以利用ALS进行求解
    
  * 目标函数为
    $$
    arg\min_{p_i q_j} \sum_{(i,j) \in K}(m_{ij} - q^T_jp_i)^2 + \lambda (||p_i||^2_2+||q_j||^2_2)
    $$
    
  
* **BiasSVD**
  
  * 在FunkSVD的基础上，考虑了**用户与商品偏好**对评分预测的影响，如乐观的用户打分偏高，质量好的商品评分也偏高等
  
  * 目标函数为
    $$
    arg\min_{p_i q_j} \sum_{(i,j) \in K}(m_{ij} - \mu - b_i - b_j - q^T_jp_i)^2 + \lambda (||p_i||^2_2+||q_j||^2_2 + ||b_i||^2_2+||b_j||^2_2)
    $$
  
    * 引入了用户和商品的偏差，以及所有记录的整体平均数
  
  * 由于不止两个待求矩阵，因此不适用ALS方法
  
    
  
* **SVD++**

  * 在BiasSVD的基础上，考虑了**用户的隐式反馈**

    * 隐式反馈：没有具体的评分，但可能有点击，浏览等行为

  * 目标函数为
    $$
    arg\min_{p_i q_j} \sum_{(i,j) \in K}(m_{ij} - \mu - b_i - b_j - q^T_jp_i - q^T_j |I(i)|^{-\frac{1}{2}} \sum_{j \in I(i)} y_j)^2 + \lambda (||p_i||^2_2+||q_j||^2_2 + ||b_i||^2_2+||b_j||^2_2 + \sum_{s \in I(i)} ||y_j||^2_2)
    $$

    * 引入了隐式反馈项

  * 也因此带来了更大的计算量，使用surprise的SVDpp相对其他两个算法明显耗时很多



## Thinking3：矩阵分解算法在推荐系统中有哪些应用场景，存在哪些不足

* **应用场景**

  * 在推荐系统中，主要应用在用户对商品的评分（或其他隐式行为如点击、页面停留时间等）等场景，通过对所得到的用户-商品的二维矩阵进行分解，并预测用户对未评分（购买等）商品的评分值并排序，从而推荐分值最高的N个商品
* **不足**
  
  * 只能应用于两个维度的数据集（如用户维度对商品维度的评分），当数据集包含的数据维度比较高时，则无法利用这些额外的信息，导致推荐效果可能不佳
  
    

## Thinking4：假设一个小说网站，有N部小说，每部小说都有摘要描述。如何针对该网站制定基于内容的推荐系统，即用户看了某部小说后，推荐其他相关的小说。原理和步骤是怎样的

* **原理**
  *  通过分析这部小说与其他小说的相似度（如作者、简介、小说分类等多个方面），对相似度进行排序，找出相似度最高的前K部小说进行推荐
* **步骤**
  1. 对小说的摘要描述进行分词，进而提取特征
     * N-Gram，提取N个连续字的集合，作为特征
     * TF-IDF，按照(min_df, max_df)提取关键词，并生成TFIDF矩阵
  2. 基于TFIDF矩阵，计算小说之间的余弦相似度矩阵
  3. 对于用户看了的某部小说，选择相似度最高的K部小说进行推荐



## Thinking5：Word2Vec的应用场景有哪些

* **适用情况**
  * 对于一个序列数据，在序列局部数据间存在着很强的关联
  * 以文本序列为例，邻近的词之间关联很强，甚至可以通过一个词的上下文大概预测出中间的词是什么
* **应用场景**
  * 在社交网络中的推荐
    *  将大V看作词，将每个用户关注大V的顺序构成语句或“文章”，利用Embedding方法来得到大V的特征表达
    * 当一个新用户初始对少数几个大V进行关注后，基于关注大V的顺序，计算这几个大V的余弦相似度，从而推荐最相似的N个大V
  * 计算商品的相似度
    *  在商品推荐的场景中，竞品推荐和搭配推荐的时候都有可能需要计算任何两个商品的相似度
    * 根据浏览/收藏/下单/App下载等行为，可以将商品看做词，将每一个用户的一类行为序看做一个文档，通过word2vec将其训练为一个向量，再计算相似度进行推荐


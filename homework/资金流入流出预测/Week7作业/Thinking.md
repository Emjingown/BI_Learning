* **Thinking1：请简述基于蒙特卡洛的强化学习的原理？**

  * AlphaGo Zero本身没有使用现成的棋谱来指导下棋，因此前期是没有足够的label来对策略价值网络进行训练的。而通过使用蒙特卡洛搜索树，可以在早期快速地进行下棋模拟，得到大量棋局及对应结果。随着存储棋盘数量的增加，MCTS得到的赢棋概率也相对更准确，此时也可以很好地对策略价值网络进行训练。

  * 基于蒙特卡洛的强化学习的原理

    * 采用蒙特卡洛树搜索（MCTS），基于UCB值进行选择、对未执行过的节点进行扩展、采用随机走子方式快速模拟大量棋局、对棋局结果进行回传等，从而可以更新树的UCB值，使预测结果更准确
    * 基于MCTS积累的棋盘数据来训练策略价值网络，将棋盘状态作为输入，使用CNN进行预测，输出落子的期望价值与采取各动作的概率
    * 基于策略价值网络得到的各动作概率作为输入，又可以继续采用MCTS方法进行自我对弈，对下期策略（即落子概率）进行优化，得到预测效果更好的落子概率，继而再继续训练更新策略价值网络，如此反复

    

* **Thinking2：强化学习的目标与深度学习的目标有何区别？**

  * 深度学习的任务通常是基于大量带有标签的数据，解决的是预测（回归或分类）问题，目标是通过利用大量数据训练深度学习模型，自动提取复杂特征，对问题背后隐含的函数进行拟合，从而当获得新的数据时，可以通过深度学习模型预测到比较准确的结果
  * 强化学习通常是将任务抽象成Agent与环境的一个交互过程，通过Agent采取某些行动，环境得到某种状态，并由此对Agent产生奖励或惩罚，Agent在这个交互过程中不断优化行动的策略，使得有更高的概率获取更高的奖励。因此，强化学习本身不依赖于大量带标签的数据，目标是基于问题规则，在大量的交互反馈中学习解决问题的策略
## Thinking与Action

* **Thinking1：机器学习中的监督学习、非监督学习、强化学习有何区别**

  * 监督学习通常用于回归与分类任务，主要特点是训练数据有标注（标签），通过挖掘特征与标签的关系，从而当新的数据进来时，根据训练的模型进行预测

  * 非监督学习的常用场景有聚类、降维等，主要特点是缺乏先验信息，训练数据没有已知标签，因此利用模型去挖掘数据间内在的关系，如聚类算法可以根据数据的相似性对数据分成不同的类别等

  * 强化学习使用的数据同样没有已知的标签，通过对问题抽象成个体与环境的交互，个体基于环境的状态采取某种动作，而环境则基于个体的动作而状态发生变化，同时可能对个体给予奖励。通过一系列的交互决策来使得个体学习到某种规律来达成任务目标。

    

* **Thinking2：什么是策略网络，价值网络，有何区别**

  * 策略网络对于给定的输入，通过学习给出一个确定输出的网络
  * 价值网络通过计算当前状态的累积分数的期望，目标是最大化这个期望值
  * 两者的区别我觉得主要是价值网络的原理有点类似贪心算法，追求的是价值的最大化，由于在每一个状态下计算得到的不同动作的价值都是确定的，导致采取的行动也是确定性的，容易陷入局部最优。而策略网络输出的是采取某个动作的概率，这个概率与受到的奖励呈正相关，具有一定的随机性，在有些场景或者比较复杂的问题可能更加适用。

  

* **Thinking3：请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的**

  1. **Selection**
     * 在树中找到一个最好的值得探索的节点，一般是**先选择未被探索的子节点**，**如果都探索过就选择UCB值最大的子节点**
  2. **Expansion**
     * 在选中的子节点中走一步创建一个新的子节点，一般是随机进行一个操作，且这个操作不能与前面的子节点重复
  3. **Simulation**
     * 在前面新Expansion出来的节点开始模拟游戏，直到到达游戏结束状态，继而得到这个新节点的得分
  4. **Backpropagation**
     * 把新节点的得分**反馈到前面所有父节点中**，更新这些节点的得分和运行次数

  

* **Thinking4：假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑**

  * 对信息流推荐中的强化学习问题进行抽象

    * 用户过去一定时间内对不同短视频产生的行为（如快速划过、点赞、收藏、分享、停留等）定义为**环境的不同状态**
    * Agent基于环境的状态（用户历史行为）进行不同短视频的推荐（即个体**可采取的不同动作**）
    * 如果推荐的短视频受用户喜欢（如产生点赞、收藏、分享、停留等行为），则视为环境**产生不同程度的奖励**，快速划过则没有奖励
    * Agent的**策略**则是最大化累积奖励的期望

  * 为避免Agent基于价值最大化一直推荐同类的短视频，可通过引入规则，或者进行同类降权来实现短视频推荐的多样化等

    

* **Thinking5：在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路**

  * 通过在不同的驾驶场景收集汽车传感器（如摄像头、测距雷达等）的信号并进一步提取特征，将汽车在路上不同状况下的状态（如安全，发生碰撞等）构建状态空间，定义好相应的奖励（比如足够安全的场景有奖励，碰撞则产生大的惩罚等），继而构建好驾驶的“环境”

  * 结合现实中汽车的性能、可执行的操作等，定义个体（即模拟的汽车）可执行的动作空间

  * 如果基于价值网络，则最大化汽车处于安全状态计算的价值来获取动作，如果是基于策略网络则输出不同动作的安全概率值，继而通过个体与环境的交互来进行模型训练

    

* **Action（五子棋）：棋盘大小 10 * 10，采用强化学习（策略价值网络），用AI训练五子棋AI，请说明都有哪些模块，不同模块的原理**

  * 训练五子棋AI的模块和围棋的比较相似，不同的主要是（1）五子棋棋盘维度比围棋的小很多，因此算法复杂度可以简化一些（如神经网络层数降低、MCTS储存的棋盘数可以减少等）；（2）获胜的规则有所不同，因此进行赢棋判断时会有所区别。

  * 因此，模块与AlphaGo的模块相似（如图），这里简单介绍一下：

    ![image-20210305125520025](D:/Jupyter_Spyder/GitHub/Study-Notes/Python数据分析/数据挖掘与高级商业分析/名企实训班/笔记/BI名企班笔记.assets/image-20210305125520025.png)

    1. **CNN模块**：将当前棋盘状态作为输入（也可以同时再输入双方前几部的棋盘状态，增加特征量，或者作为时间序列来处理也行），基于以下损失函数来训练策略价值网络
       $$
       Loss={{\left( z-v \right)}^{2}}-{{\pi }^{T}}\log P + c{{\theta }^{2}}
       $$

       使得网络兼顾对落子概率以及棋盘赢棋期望（价值）的预测

    2. **MCTS模块**：将当前棋盘状态作为输入，根据已存储的棋盘信息，通过计算UCB值等进行节点选择，对未走过的落棋位置进行扩展、多次模拟并将结果回传父节点，更新MCTS存储的信息，继而输出落子概率分布
    3. **自对弈模块**：通过利用AI自己与自己对弈的方式来提高网络训练效率，通过计算各自的最佳落子方式来进行落子，继而棋盘更新到新的状态，再作为CNN模块的输入，循环训练。
    4. **可能减少训练次数的措施**：我自己再去体验了几把五子棋（被虐..），发现落子基本都是在有己方旗子位置两三格范围内（由于五子棋连着5颗就获胜，下在没己方旗子的位置只会加快输旗），通过加入这样可靠的先验知识可以减少不必要模拟的位置，从而大大降低训练难度。